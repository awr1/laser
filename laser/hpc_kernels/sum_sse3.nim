# Laser
# Copyright (c) 2018 Mamy André-Ratsimbazafy
# Distributed under the Apache v2 License (license terms are at http://www.apache.org/licenses/LICENSE-2.0).
# This file may not be copied, modified, or distributed except according to those terms.

import
  ../simd, ../compiler_optim_hints,
  ./private/[align_unroller, sse3_utils]

func sum_sse3*(data: ptr UncheckedArray[float32], len: Natural): float32 =
  ## Sum a contiguous range of float32 using SSE3 instructions

  # Loop peeling, while not aligned to 16-byte boundary advance
  var idx = 0
  while (cast[ByteAddress](data) and 15) != 0:
    result += data[idx]
    inc idx

  let data_aligned = assume_aligned cast[ptr UncheckedArray[float32]](data[idx].addr)

  # Main vectorized and unrolled loop. We unroll it twice as
  # from Core 2 to Broadwell, latency is 3, throughput is 1 for FPadd
  # and we have additional cycles for:
  #   - loading data
  #   - incrementing the loop
  #   - testing the exit condition (free with branch prediction)
  #
  # Testing with unrolling 4 times shows that we run out of registers
  # as an extra movaps is inserted by the compiler.
  # Unrolling 3 times doesn't improve speed as well,
  # we might be bottlenecking on µop ports.
  #
  # Note on Skylake+ and AMD latency/throughput:
  # - Piledriver: latency 5-6, throughput 0.5 NOK
  # - Ryzen: latency 3, throughput 0.5 OK
  # - Skylake: latency 4, throughput 0.5 NOK
  const step = 8
  let new_end = len - idx
  let unroll_stop = round_step_down(new_end, step)
  var
    accum4_0 = mm_setzero_ps()
    accum4_1 = mm_setzero_ps()

  for i in countup(0, unroll_stop - 1, step):
    let
      data4_0 = data_aligned[i  ].addr.mm_load_ps()
      data4_1 = data_aligned[i+4].addr.mm_load_ps()
    accum4_0 = mm_add_ps(accum4_0, data4_0)
    accum4_1 = mm_add_ps(accum4_1, data4_1)
  accum4_0 = mm_add_ps(accum4_0, accum4_1)
  for i in unroll_stop ..< new_end:
    result += data_aligned[i]
  result += accum4_0.sum_ps_sse3()

  ## Loop generated by Clang
  # +0x67	nopw                (%rax,%rax)
  # +0x70	    vaddps              (%rdi,%rdx,4), %xmm0, %xmm0
  # +0x75	    vaddps              16(%rdi,%rdx,4), %xmm1, %xmm1
  # +0x7b	    vaddps              32(%rdi,%rdx,4), %xmm0, %xmm0
  # +0x81	    vaddps              48(%rdi,%rdx,4), %xmm1, %xmm1
  # +0x87	    vaddps              64(%rdi,%rdx,4), %xmm0, %xmm0
  # +0x8d	    vaddps              80(%rdi,%rdx,4), %xmm1, %xmm1   # Bottleneck here or the previous instr
  # +0x93	    vaddps              96(%rdi,%rdx,4), %xmm0, %xmm0
  # +0x99	    vaddps              112(%rdi,%rdx,4), %xmm1, %xmm1
  # +0x9f	    addq                $32, %rdx
  # +0xa3	    addq                $4, %rax
  # +0xa7	    jne                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x70"
