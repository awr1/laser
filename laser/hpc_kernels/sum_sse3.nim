# Laser
# Copyright (c) 2018 Mamy Andr√©-Ratsimbazafy
# Distributed under the Apache v2 License (license terms are at http://www.apache.org/licenses/LICENSE-2.0).
# This file may not be copied, modified, or distributed except according to those terms.

import
  ../simd, ../compiler_optim_hints,
  ./private/[align_unroller, sse3_utils]


func sum_sse3*(data: ptr UncheckedArray[float32], len: Natural): float32 =
  ## Sum a contiguous range of float32 using SSE3 instructions

  # Loop peeling, while not aligned to 16-byte boundary advance
  var idx = 0
  while (cast[ByteAddress](data) and 15) != 0:
    result += data[idx]
    inc idx

  let data_aligned = assume_aligned cast[ptr UncheckedArray[float32]](data[idx].addr)

  # Main vectorized and unrolled loop. We unroll it 3 times as
  # from Core 2 to Broadwell, latency is 3, throughput is 1 for FP add
  #
  # Note on Skylake+ and AMD latency/throughput:
  # - Piledriver: latency 5-6, throughput 0.5 NOK
  # - Ryzen: latency 3, throughput 0.5 OK
  # - Skylake: latency 4, throughput 0.5 NOK
  #
  # At the end of each loop there is an addition to the loop index
  # and a comparison to get out of the loop. So the extra latency
  # on Skylake is not an issue.
  let new_end = len - idx
  let unroll_stop = round_step_down(new_end, 12)
  var
    accum4_0 = mm_setzero_ps()
    accum4_1 = mm_setzero_ps()
    accum4_2 = mm_setzero_ps()

  for i in countup(0, unroll_stop - 1, 12):
    let
      data4_0 = data_aligned[i  ].addr.mm_load_ps()
      data4_1 = data_aligned[i+4].addr.mm_load_ps()
      data4_2 = data_aligned[i+8].addr.mm_load_ps()
    accum4_0 = mm_add_ps(accum4_0, data4_0)
    accum4_1 = mm_add_ps(accum4_1, data4_1)
    accum4_2 = mm_add_ps(accum4_2, data4_2)

  accum4_0 = mm_add_ps(accum4_0, accum4_1)
  accum4_0 = mm_add_ps(accum4_0, accum4_2)
  for i in unroll_stop ..< new_end:
    result += data_aligned[i]
  result += accum4_0.sum_ps_sse3()

  ## Loop generated by Clang
  # +0x42	vxorps              %xmm0, %xmm0, %xmm0
  # +0x46	xorl                %edx, %edx
  # +0x48	vxorps              %xmm2, %xmm2, %xmm2
  # +0x4c	vxorps              %xmm1, %xmm1, %xmm1
  # +0x50	    vaddps              (%rdi,%rdx,4), %xmm0, %xmm0
  # +0x55	    vaddps              16(%rdi,%rdx,4), %xmm2, %xmm2
  # +0x5b	    vaddps              32(%rdi,%rdx,4), %xmm1, %xmm1
  # +0x61	    addq                $12, %rdx
  # +0x65	    cmpq                %rax, %rdx
  # +0x68	    jl                  "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x50"
  # +0x6a	jmp                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x78"
  # +0x6c	vxorps              %xmm1, %xmm1, %xmm1
