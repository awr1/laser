# Laser
# Copyright (c) 2018 Mamy André-Ratsimbazafy
# Distributed under the Apache v2 License (license terms are at http://www.apache.org/licenses/LICENSE-2.0).
# This file may not be copied, modified, or distributed except according to those terms.

import
  ../simd, ../compiler_optim_hints,
  ./private/[align_unroller, sse3_utils]

func sum_sse3*(data: ptr UncheckedArray[float32], len: Natural): float32 =
  ## Sum a contiguous range of float32 using SSE3 instructions

  # Loop peeling, while not aligned to 16-byte boundary advance
  var idx = 0
  while (cast[ByteAddress](data) and 15) != 0:
    result += data[idx]
    inc idx

  let data_aligned = assume_aligned cast[ptr UncheckedArray[float32]](data[idx].addr)

  # Main vectorized and unrolled loop. We unroll it twice as
  # from Core 2 to Broadwell, latency is 3, throughput is 1 for FPadd
  # and we have additional cycles for:
  #   - loading data
  #   - incrementing the loop
  #   - testing the exit condition (free with branch prediction)
  #
  # Testing with unrolling 4 times shows that we run out of registers
  # as an extra movaps is inserted by the compiler.
  # Unrolling 3 times doesn't improve speed as well,
  # we might be bottlenecking on µop ports.
  #
  # Note on Skylake+ and AMD latency/throughput:
  # - Piledriver: latency 5-6, throughput 0.5 NOK
  # - Ryzen: latency 3, throughput 0.5 OK
  # - Skylake: latency 4, throughput 0.5 NOK

  let new_end = len - idx
  let unroll_stop = round_step_down(new_end, 8)
  var
    accum4_0 = mm_setzero_ps()
    accum4_1 = mm_setzero_ps()

  for i in countup(0, unroll_stop - 1, 12):
    let
      data4_0 = data_aligned[i  ].addr.mm_load_ps()
      data4_1 = data_aligned[i+4].addr.mm_load_ps()
    accum4_0 = mm_add_ps(accum4_0, data4_0)
    accum4_1 = mm_add_ps(accum4_1, data4_1)
  accum4_0 = mm_add_ps(accum4_0, accum4_1)
  for i in unroll_stop ..< new_end:
    result += data_aligned[i]
  result += accum4_0.sum_ps_sse3()

  ## Loop generated by Clang - with AVX
  # +0x28	nopl                (%rax,%rax)
  # +0x30	    vaddps              (%rdi,%rcx,4), %xmm0, %xmm0
  # +0x35	    vaddps              16(%rdi,%rcx,4), %xmm1, %xmm1
  # +0x3b	    addq                $12, %rcx
  # +0x3f	    cmpq                %rax, %rcx
  # +0x42	    jl                  "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x30"
  # +0x44	jmp                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x4e"

  ## Without
  # +0x26	nopw                %cs:(%rax,%rax)
  # +0x30	    addps               (%rdi,%rcx,4), %xmm1
  # +0x34	    addps               16(%rdi,%rcx,4), %xmm0
  # +0x39	    addq                $12, %rcx
  # +0x3d	    cmpq                %rax, %rcx
  # +0x40	    jl                  "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x30"
  # +0x42	jmp                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x4a"
