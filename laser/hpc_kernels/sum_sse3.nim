# Laser
# Copyright (c) 2018 Mamy Andr√©-Ratsimbazafy
# Distributed under the Apache v2 License (license terms are at http://www.apache.org/licenses/LICENSE-2.0).
# This file may not be copied, modified, or distributed except according to those terms.

import
  ../simd, ../compiler_optim_hints,
  ./private/[align_unroller, sse3_utils]

func sum_sse3*(data: ptr UncheckedArray[float32], len: Natural): float32 =
  ## Sum a contiguous range of float32 using SSE3 instructions
  withCompilerOptimHints()
  let data{.restrict.} = data

  # Loop peeling, while not aligned to 16-byte boundary advance
  var idx = 0
  while (cast[ByteAddress](data) and 15) != 0:
    result += data[idx]
    inc idx

  let data_aligned{.restrict.} = assume_aligned cast[ptr UncheckedArray[float32]](data[idx].addr)

  # Main vectorized and unrolled loop.
  const step = 16
  let new_end = len - idx
  let unroll_stop = round_step_down(new_end, step)
  var
    accum4_0 = mm_setzero_ps()
    accum4_1 = mm_setzero_ps()
    accum4_2 = mm_setzero_ps()
    accum4_3 = mm_setzero_ps()

  for i in countup(0, unroll_stop - 1, step):
    let
      data4_0 = data_aligned[i   ].addr.mm_load_ps()
      data4_1 = data_aligned[i+4 ].addr.mm_load_ps()
      data4_2 = data_aligned[i+8 ].addr.mm_load_ps()
      data4_3 = data_aligned[i+12].addr.mm_load_ps()
    accum4_0 = mm_add_ps(accum4_0, data4_0)
    accum4_1 = mm_add_ps(accum4_1, data4_1)
    accum4_2 = mm_add_ps(accum4_2, data4_2)
    accum4_3 = mm_add_ps(accum4_3, data4_3)
  accum4_0 = mm_add_ps(accum4_0, accum4_1)
  accum4_2 = mm_add_ps(accum4_2, accum4_3)
  accum4_0 = mm_add_ps(accum4_0, accum4_2)
  for i in unroll_stop ..< new_end:
    result += data_aligned[i]
  result += accum4_0.sum_ps_sse3()

  ## Loop generated by Clang - memory bandwith bottleneck after 64 Bytes are loaded?
  # +0x4c	nopl                (%rax)
  # +0x50	    vaddps              (%rdi,%rcx,4), %xmm0, %xmm0
  # +0x55	    vaddps              16(%rdi,%rcx,4), %xmm1, %xmm1
  # +0x5b	    vaddps              32(%rdi,%rcx,4), %xmm2, %xmm2
  # +0x61	    vaddps              48(%rdi,%rcx,4), %xmm3, %xmm3
  # +0x67	    vaddps              64(%rdi,%rcx,4), %xmm0, %xmm0
  # +0x6d	    vaddps              80(%rdi,%rcx,4), %xmm1, %xmm1 # Bottleneck (or previous instruction)
  # +0x73	    vaddps              96(%rdi,%rcx,4), %xmm2, %xmm2
  # +0x79	    vaddps              112(%rdi,%rcx,4), %xmm3, %xmm3
  # +0x7f	    addq                $32, %rcx
  # +0x83	    addq                $2, %rdx
  # +0x87	    jne                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x50"

